{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_squadshifts_aggregated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mt-4mWIOeSQ-VM-6ETt6EKoZLrZmS1xz",
      "authorship_tag": "ABX9TyO1Nbc3wq5g9ovHWl73M2OQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rocabrera/language-uncertainty/blob/master/create_squadshifts_aggregated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pandas"
      ],
      "metadata": {
        "id": "4sBQqKhfQa3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "fhgc0_ZjQVs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgm-Wv-DQJn0"
      },
      "outputs": [],
      "source": [
        "df = (pd.read_csv('/content/drive/MyDrive/UNICAMP/scored_squadshifts_paraphrased.csv')\n",
        "        .dropna()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(predict_text: str, label_text:str):\n",
        "    pred_tokens = predict_text.split()\n",
        "    truth_tokens = label_text.split()\n",
        "    \n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    \n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "\n",
        "def custom_f1s(x):\n",
        "  predicted_answer = x[\"predicted_answer\"]\n",
        "  answers: dict = eval(x[\"answers\"])\n",
        "  f1s = [compute_f1(predicted_answer, answer) for answer in answers[\"text\"]]\n",
        "  return f1s, max(f1s)\n",
        "\n",
        "df[\"f1s\"], df[\"max_f1\"] = zip(*df.apply(custom_f1s, axis=1))"
      ],
      "metadata": {
        "id": "xkqMbVKgQSV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_mean = df.groupby(\"id\", as_index=False).agg({\"max_f1\":[\"mean\"]}).droplevel(level=1, axis=1).rename(columns={\"max_f1\":\"mean_f1\"})"
      ],
      "metadata": {
        "id": "YDrnlV3MQfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df = (pd.read_csv('/content/drive/MyDrive/UNICAMP/squadshifts_original.csv')\n",
        "                 .dropna()\n",
        "                 .merge(id_mean, on=\"id\")\n",
        ")"
      ],
      "metadata": {
        "id": "TpYbxu17QivB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df[\"uncertainty\"] = round(1 - original_df[\"mean_f1\"], 2)"
      ],
      "metadata": {
        "id": "nf0CxrB2QkRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_not_bucket_uncertainty_label(x):\n",
        "\n",
        "  uncertainty = x[\"uncertainty\"]\n",
        "  answers: dict = eval(x[\"answers\"])\n",
        "  true_labels = [f\"{answer} Uncertainty: {uncertainty}\" for answer in answers[\"text\"]]\n",
        "  return {\"text\": true_labels}\n",
        "\n",
        "\n",
        "original_df[\"answers_not_bucket_uncertainty\"] = original_df.apply(create_not_bucket_uncertainty_label, axis=1)"
      ],
      "metadata": {
        "id": "tuvGVL4pQlqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df[\"answers_not_bucket_uncertainty\"].iloc[0], original_df[\"answers_not_bucket_uncertainty\"].iloc[194] \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz9DtZlWGLgI",
        "outputId": "94b5bc1f-85c6-4199-c9b0-fec1eb98e0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'text': ['Each brotherhood elects two delegates who take part in the National Ecclesiastical Assembly Uncertainty: 0.61',\n",
              "   'two delegates Uncertainty: 0.61',\n",
              "   'two delegates Uncertainty: 0.61',\n",
              "   'two delegates Uncertainty: 0.61']},\n",
              " {'text': ['initial letters Uncertainty: 0.0',\n",
              "   'an abbreviation Uncertainty: 0.0',\n",
              "   'any abbreviation formed from initial letters Uncertainty: 0.0']})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mpPGAGOQFkvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = pd.IntervalIndex.from_tuples([(0.0, 0.32), (0.33, 0.65), (0.66, 1.)], closed=\"both\")\n",
        "bucket_uncertainty = pd.cut(original_df[\"uncertainty\"], bins=bins)\n",
        "print(bucket_uncertainty.cat.categories)\n",
        "bucket_uncertainty.cat.categories = [\"low\", \"medium\", \"high\"]\n",
        "print(bucket_uncertainty.cat.categories)\n",
        "original_df[\"bucket_uncertainty\"] = bucket_uncertainty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8bHDslssw_t",
        "outputId": "ab4b8dbe-8ed0-4dae-8162-cf8436d37eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IntervalIndex([[0.0, 0.32], [0.33, 0.65], [0.66, 1.0]], dtype='interval[float64, both]')\n",
            "Index(['low', 'medium', 'high'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bucket_uncertainty_label(x):\n",
        "\n",
        "  uncertainty = x[\"bucket_uncertainty\"]\n",
        "  answers: dict = eval(x[\"answers\"])\n",
        "  true_labels = [f\"{answer} Uncertainty: {uncertainty}\" for answer in answers[\"text\"]]\n",
        "  return {\"text\": true_labels}\n",
        "\n",
        "original_df[\"answers_bucket_uncertainty\"] = original_df.apply(create_bucket_uncertainty_label, axis=1)"
      ],
      "metadata": {
        "id": "cF5vTgusFZFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "original_df[\"answers_bucket_uncertainty\"].iloc[0], original_df[\"answers_bucket_uncertainty\"].iloc[194] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdhWLG9IGGdJ",
        "outputId": "f3049ee9-4e79-44e6-cce9-6255af6d043d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'text': ['Each brotherhood elects two delegates who take part in the National Ecclesiastical Assembly Uncertainty: medium',\n",
              "   'two delegates Uncertainty: medium',\n",
              "   'two delegates Uncertainty: medium',\n",
              "   'two delegates Uncertainty: medium']},\n",
              " {'text': ['initial letters Uncertainty: low',\n",
              "   'an abbreviation Uncertainty: low',\n",
              "   'any abbreviation formed from initial letters Uncertainty: low']})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "9cOU2ckeFasv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(df:pd.DataFrame, approximated_train_pct:float, approximated_eval_pct:float) -> List[pd.DataFrame]:\n",
        "\n",
        "  df = df.sample(frac=1)\n",
        "  df[\"context_codes\"] = df[\"context\"].astype(\"category\").cat.codes\n",
        "  \n",
        "  dataset_max = df[\"context_codes\"].max()\n",
        "  max_train_idx = int(np.ceil(dataset_max*approximated_train_pct))\n",
        "  train_df = df.query(f\"context_codes<={max_train_idx}\").copy()\n",
        "  aux =  df.query(f\"context_codes > {max_train_idx}\").copy()\n",
        "  max_eval_index = int(np.ceil(max_train_idx + (dataset_max - max_train_idx)*approximated_eval_pct))\n",
        "  eval_df = aux.query(f\"context_codes<={max_eval_index}\").copy()\n",
        "  test_df =  aux.query(f\"context_codes > {max_eval_index}\").copy()\n",
        "\n",
        "  return train_df, eval_df, test_df\n",
        "\n",
        "approximated_eval_pct = 0.5\n",
        "approximated_train_pct = 0.85\n",
        "train_df, eval_df, test_df = split_dataset(original_df, approximated_train_pct, approximated_eval_pct)"
      ],
      "metadata": {
        "id": "i7fhIDRV5mZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Temos aproximadamente {round(approximated_train_pct,3)} do dataset para treino\")\n",
        "print(f\"Train Percentage: {round(len(train_df)/len(original_df),3)}\")\n",
        "print(f\"Temos aproximadamente {round(1-approximated_train_pct,3)} do dataset para separar entre test e validação com porcentagem {approximated_eval_pct} para validacao.\")\n",
        "print(f\"Eval Percentage: {round(len(eval_df)/len(original_df),3)}\")\n",
        "print(f\"Test Percentage: {round(len(test_df)/len(original_df),3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEWrlyyR8d9z",
        "outputId": "7d0bf211-6d5b-45be-be3f-a86ba293bf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temos aproximadamente 0.85 do dataset para treino\n",
            "Train Percentage: 0.846\n",
            "Temos aproximadamente 0.15 do dataset para separar entre test e validação com porcentagem 0.5 para validacao.\n",
            "Eval Percentage: 0.081\n",
            "Test Percentage: 0.074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mostrando que não tem intersecção de contexto nos datasets\")\n",
        "print(set(train_df.context_codes.unique()).intersection(test_df.context_codes.unique()))\n",
        "print(set(train_df.context_codes.unique()).intersection(eval_df.context_codes.unique()))\n",
        "print(set(eval_df.context_codes.unique()).intersection(test_df.context_codes.unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvlF4gFUAWe0",
        "outputId": "88718b4c-75ce-4c19-dd8d-248b0450fe6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mostrando que não tem intersecção de contexto nos datasets\n",
            "set()\n",
            "set()\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"/content/squadshifts_aggregated_train.csv\", index=False)\n",
        "eval_df.to_csv(\"/content/squadshifts_aggregated_eval.csv\", index=False)\n",
        "test_df.to_csv(\"/content/squadshifts_aggregated_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "06XjqKMWQoBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}