{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af115b7-24fa-4e8a-ac3f-4a26aec96b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from termcolor import colored\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils import sample_example_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d930f40f-ace3-42a1-839c-44b22ef969ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/rocabrera/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33a1e891674a9fb629c7f678c89afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"squad\")\n",
    "train, validation = data[\"train\"], data[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49790799-9e7a-4d21-9587-808b5faf795c",
   "metadata": {},
   "source": [
    "> The Stanford Question Answering Dataset (SQuAD) is a famous dataset that is often used to test the ability of machines to read a passage of text and answer questions about it. The dataset was created by sampling several hundred English articles from Wikipedia, partitioning each article into paragraphs, and then asking crowdworkers to generate a set of questions and answers for each paragraph. In the first version of SQuAD, each answer to a question was guaranteed to exist in the corresponding passage. But it wasn’t long before sequence models started performing better than humans at extracting the correct span of text with the answer. To make the task more difficult, SQuAD 2.0 was created by augmenting SQuAD 1.1 with a set of adversarial questions that are relevant to a given passage but cannot be answered from the text alone.\n",
    "\n",
    "Ref: Capitulo 7 - Natural language Processing with Transformers\n",
    "\n",
    "O dataset que estamos utilizando é do ano de 2016, o qual parece ser a versão 1.x com toda pergunta tendo uma resposta para ser extraida do contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e807e3d-8d7c-4dd1-a570-a6b523b13b4e",
   "metadata": {},
   "source": [
    "---\n",
    "**Toda pergunta tem exatamente uma resposta.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a458584-ec0b-423d-962d-0cb5f2e45702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: True if len(x[\"text\"]) != 1 else False, train[\"answers\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a532c8-445f-4ff1-a20d-ec38de857cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: True if len(x[\"text\"]) == 1 else False, train[\"answers\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669efa3-ce79-4243-b6d5-14b1b3fe030a",
   "metadata": {},
   "source": [
    "---\n",
    "**Apresentando Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592898ec-7252-425d-8e77-c5b77df310df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mWhere is the headquarters of the Congregation of the Holy Cross?\u001b[0m\u001b[32mThe university is the major seat of the\n",
      "Congregation of Holy Cross (albeit not its official headquarters, which are in \u001b[0m\u001b[34mRome\u001b[0m\u001b[32m). Its main\n",
      "seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the\n",
      "oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests\n",
      "and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the\n",
      "Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic,\n",
      "Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.\u001b[0m\n",
      "\u001b[31mTRUE LABEL: Rome\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = sample_example_dataset(data=train, idx=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_huggingface",
   "language": "python",
   "name": "venv_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
